#  Mise en place d‚Äôune infrastructure de donn√©es m√©t√©orologiques ‚Äì Forecast 2.0

**Forecast 2.0** est un projet de **Data Engineering** r√©alis√© pour **GreenCoop**, une entreprise sp√©cialis√©e dans la gestion et la production d‚Äô√©nergie renouvelable.  
L‚Äôobjectif est de fournir **quotidiennement des donn√©es m√©t√©orologiques de qualit√©** aux Data Scientists afin d‚Äôam√©liorer leurs **mod√®les de pr√©vision de la demande en √©lectricit√©**.

En tant que **Data Engineer**, j‚Äôai con√ßu et d√©ploy√© un **pipeline complet** permettant d‚Äôint√©grer, transformer, stocker et s√©curiser des donn√©es issues de **plusieurs sources m√©t√©orologiques** (stations officielles et amateurs).

---

##  Objectifs

- Int√©grer de nouvelles sources de donn√©es m√©t√©o *(InfoClimat, Weather Underground, etc.)*  
- Mettre en place une **infrastructure cloud scalable** *(AWS + Docker)*  
- Garantir la **qualit√© et la coh√©rence** des donn√©es stock√©es  
- Offrir une **connexion directe** pour les Data Scientists via MongoDB *(utilis√© par SageMaker)*  

---

## Architecture du pipeline

Le pipeline de donn√©es **Forecast 2.0** se compose de plusieurs √©tapes :

### 1Ô∏è‚É£ Extraction ‚Äì *Airbyte + S3*
- Connexion aux sources m√©t√©o *(Excel, JSON)*  
- Chargement automatique dans un bucket **AWS S3**

### 2Ô∏è‚É£ Transformation ‚Äì *Python*
- Nettoyage, typage et harmonisation des colonnes  
- Contr√¥les qualit√© *(valeurs manquantes, coh√©rence, doublons)*

### 3Ô∏è‚É£ Chargement ‚Äì *MongoDB*
- Migration des donn√©es depuis **S3** vers **MongoDB**  
- Cr√©ation de collections adapt√©es aux diff√©rents types de stations  
- V√©rification automatique de la conformit√© post-migration

### 4Ô∏è‚É£ Conteneurisation & D√©ploiement ‚Äì *Docker + AWS ECS*
- Conteneurisation du script Python et de MongoDB  
- D√©ploiement sur **Amazon ECS** avec monitoring via **CloudWatch**

---

## ‚öôÔ∏è Stack technique

| Domaine | Outil / Service |
|----------|-----------------|
| **Extraction** | Airbyte |
| **Stockage interm√©diaire** | AWS S3 |
| **Transformation** | Python *(pandas, boto3, pymongo)* |
| **Base de donn√©es** | MongoDB |
| **Conteneurisation** | Docker, Docker Compose |
| **Cloud** | AWS *(ECS, S3, CloudWatch)* |
| **Documentation** | Markdown, README, logigramme du pipeline |

---

##  S√©curit√© et bonnes pratiques

- Variables sensibles stock√©es dans un fichier **`.env`** *(non versionn√©)*  
- Authentification MongoDB via un utilisateur restreint **`forecast_writer`**  
- Connexions AWS s√©curis√©es via **cl√©s IAM**  
- Sauvegardes automatiques des donn√©es sur AWS  

---

##  Contr√¥le qualit√©

- Suppression des lignes incompl√®tes ou incoh√©rentes  
- V√©rification du sch√©ma avant et apr√®s migration  
- Rapport automatique sur :
  - le taux de valeurs manquantes  
  - la correspondance des colonnes entre la source et MongoDB  

---

##  R√©sultats

- Pipeline **automatis√© de bout en bout**  
- Temps d‚Äôacc√®s moyen aux donn√©es : **< 2 secondes** pour une requ√™te standard  
- Int√©gration fluide avec **SageMaker** pour les mod√®les de pr√©vision  

---

##  Auteur

üë§ **Khalid OURO-ADOYI**  
üìß khalidouroadoyi@gmail.com  

---


